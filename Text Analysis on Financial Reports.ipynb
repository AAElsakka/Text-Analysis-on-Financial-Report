{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction and Text Analysis\n",
    "<b>Objective is to extract textual data from SEC / EDGAR financial reports and perform text analysis to compute variables that are explained below.<br><br>\n",
    "<b>Link to SEC / EDGAR financial reports are given in excel spreadsheet “cik_list.xlsx”. we add https://www.sec.gov/Archives/ to every cells of column F (cik_list.xlsx) to access link to the financial report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  \n",
       "0  edgar/data/3662/0000950170-98-000413.txt  \n",
       "1  edgar/data/3662/0000950170-98-001001.txt  \n",
       "2  edgar/data/3662/0000950172-98-000783.txt  \n",
       "3  edgar/data/3662/0000950170-98-002145.txt  \n",
       "4  edgar/data/3662/0000950172-98-001203.txt  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading data\n",
    "df = pd.read_excel(\"cik_list.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding https://www.sec.gov/Archives/ to every cells of column F and storing them in urls\n",
    "pre_url = 'https://www.sec.gov/Archives/'\n",
    "urls = list(df['SECFNAME'].apply(lambda x: pre_url+str(x)))[:152] # 152 number of urls\n",
    "total_urls = len(urls)\n",
    "#print('Total', total_urls)\n",
    "#print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stop Words Lists are used to clean the text so that Sentiment Analysis can be performed by excluding the words found in Stop Words List.<br> The Stopwords were downloaded from here https://drive.google.com/file/d/0B4niqV00F3mseWZrUk1YMGxpVzQ/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the stopwords\n",
    "with open('StopWords_Generic.txt','r') as f:\n",
    "    stopwords = f.readlines()\n",
    "    stopwords = [word.strip() for word in stopwords]\n",
    "#print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be calculating several scores such as:\n",
    "1. Positive Score: This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n",
    "2. Negative Score: This score is calculated by assigning the value of +1 for each word if found in the Negative Dictionary and then adding up all the values.\n",
    "3. Polarity Score: This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula: Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001) Range is from -1 to +1\n",
    "###### Analysis of Readability:\n",
    "4. Average Sentence Length = the number of words / the number of sentences\n",
    "5. Percentage of Complex words = the number of complex words / the number of words \n",
    "6. Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words) (Analysis of Readability is calculated using the Fox index)\n",
    "7. Average Number of Words Per Sentence = the total number of words / the total number of sentences\n",
    "8. Complex Word Count (Complex words are words in the text that contain more than two syllables.)\n",
    "9. Word Count\n",
    "10. uncertainty_score: This score is calculated by assigning the value of +1 for each word if found in the Uncertain Dictionary and then adding up all the values.\n",
    "11. constraining_score: This score is calculated by assigning the value of +1 for each word if found in the Constraining Dictionary and then adding up all the values.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving Positive words\n",
    "positive_words = pd.read_excel('LoughranMcDonald_SentimentWordLists_2018.xlsx',\n",
    "                                header=None, sheet_name='Positive')\n",
    "#display(positive_words.head())\n",
    "positive_words = list(positive_words[0].apply(lambda x : x.lower()))\n",
    "#print(positive_words[:5])\n",
    "\n",
    "#saving Negative words\n",
    "negative_words = pd.read_excel('LoughranMcDonald_SentimentWordLists_2018.xlsx',\n",
    "                                header=None, sheet_name='Negative')\n",
    "#display(negative_words.head())\n",
    "negative_words = list(negative_words[0].apply(lambda x : x.lower()))\n",
    "#print(negative_words[:5])\n",
    "\n",
    "#loading uncertainty words\n",
    "uncertaintywords = pd.read_excel('uncertainty_dictionary.xlsx')\n",
    "uncertaintywords = list(uncertaintywords['Word'].apply(lambda x:x.lower()))\n",
    "\n",
    "#loading constraining words\n",
    "constrainingwords = pd.read_excel('constraining_dictionary.xlsx')\n",
    "constrainingwords = list(constrainingwords['Word'].apply(lambda x:x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "from nltk import word_tokenize\n",
    "text = \"This is a wonderful sentence.\"\n",
    "text_words = word_tokenize(text)\n",
    "\n",
    "#print(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requesting text from web\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# keys of our scores dictionary\n",
    "keys = ['positive_score', 'negative_score', 'polarity_score',\n",
    "        'average_sentence_length', 'percentage_of_complex_words',\n",
    "        'complex_word_count', 'fog_index', 'word_count', 'uncertainty_score',\n",
    "        'constraining_score', 'positive_word_proportion', 'negative_word_proportion',\n",
    "        'uncertainty_word_proportion', 'constraining_word_proportion',\n",
    "        'constraining_words_whole_report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    #get the report\n",
    "    \n",
    "    req = requests.get(url)\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    #print(soup.prettify())\n",
    "\n",
    "    # Cleaning the soup \n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    #print(text)\n",
    "\n",
    "    #clean sentence to get text only\n",
    "    report = clean_sentencs(text, stopwords)\n",
    "\n",
    "    # Dictionary of our score\n",
    "    variables = dict.fromkeys(keys)\n",
    "\n",
    "    positive_score = calculate_score(positive_words, report, 'positive_score', variables)\n",
    "    negative_score = calculate_score(negative_words, report, 'negative_score', variables)\n",
    "    polarity_score = polarity(positive_score,negative_score, variables)\n",
    "\n",
    "    print('row {} total words: {} , total sentences {}'.format(i, len(report),len(text.split('. '))))\n",
    "\n",
    "    average_sentence_length = len(report) / len(text.split('. '))\n",
    "    variables['average_sentence_length'] = average_sentence_length\n",
    "    percentage_of_complex_words = complex_words(report, variables)\n",
    "    fog_index = fog(variables)\n",
    "    variables['word_count'] = len(report)\n",
    "\n",
    "    #calculating uncertainity_score and constraining_score\n",
    "    uncertainty_score = calculate_score(uncertaintywords, report, 'uncertainty_score',variables)\n",
    "    constraining_score = calculate_score(constrainingwords, report, 'constraining_score',variables)\n",
    "\n",
    "    positive_word_proportion = word_proportion('positive_word_proportion', report, 'positive_score', variables)\n",
    "    negative_word_proportion = word_proportion('negative_word_proportion', report, 'negative_score', variables)\n",
    "    uncertainty_word_proportion = word_proportion('uncertainty_word_proportion', report, 'uncertainty_score', variables)\n",
    "    constraining_word_proportion = word_proportion('constraining_word_proportion', report, 'constraining_score', variables)\n",
    "    variables['constraining_words_whole_report'] = variables['constraining_score']\n",
    "\n",
    "    for key in variables.keys():\n",
    "        df.loc[i,key] = variables[key]  \n",
    "\n",
    "\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice multiple duplicated rows, this occurs when i get a response of \"Your Request Originates from an Undeclared Automated Tool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "df.to_csv('sumbission.csv')\n",
    "\n",
    "#print(variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
